# The Bag-of-words approach

##  Introduction to Natural Language Processing

And its computational representation

The notion of corpus

- Coherent **collection of documents** (e.g. recipes, encyclopaedias, news, etc.)
- If we enter a few **keywords** , what is the **most relevant document**?
- This is at the core of e.g. Google search
- One way to solve the problem is **represent documents as vectors over vocabularies**
- This **breaks grammar relationships** , reducing information to **word frequencies**
- This is known as the **Bag of Words approach**


Matrix representation of a corpus

- Each column is a document,^
- Rows are the words, in the corpus **vocabulary**
- Matrix cell quantifies importance in^
**- But what quantity captures a good notion of**
    **importance?**

```
dj
wi
wi dj word_
word_2word_
word_4word_
word_6word_
word_
```
```
doc_1doc_2doc_3doc_4doc_5doc_6doc_7doc_8doc_
```
```
LowVery low
MediumMedium high
High
```

Matrix representation of a corpus

- **Word counts?** Yes but not enough
- **Problem 1:** documents have different sizes; bigger documents will bias corpus
- **Solution 1:** normalise word counts by document size
- ( is total number of words in )
- **Problem 2:** TF captures relevance of word in document, but not in corpus
- **Example:** A high TF word that appears everywhere in the corpus is not informative

_TF_ ( _wi_ , _dj_ ) = count( _wi_ , _dj_ )/| _dj_ | | _dj_ | _dj_

```
word_
word_
word_
word_
word_
word_
word_
word_
```
```
doc_1doc_2doc_3doc_4doc_5doc_6doc_7doc_8doc_
```
```
Very low
Low
Medium
Medium high
High
```

Matrix representation of a corpus

- To quantify importance of in corpus that contains documents we use the notion of
    **Inverse Document Frequency** (IDF)
       , where means number of documents that contain
- How do we interpret this quantity?
- Low values correspond to very common words, high values to very rare words
- So, what we put in our matrix is

_wi C N_

_IDF_ ( _wi_ , _C_ ) = _log_ ( _dfN_ ( _wi_ ) ) _df_ ( _wi_ ) _wi_

_ai_ , _j_ = _TF_ ( _wi_ , _dj_ ) × _IDF_ ( _wi_ )

```
word_
word_
word_
word_
word_
word_
word_
word_
```
```
doc_1doc_2doc_3doc_4doc_5doc_6doc_7doc_8doc_
```
```
Very low
Low
Medium
Medium high
High
```

Basic corpus pre-processing

1. **Instantiate the Corpus:** a collection of strings, each corresponding to a document
2. **Preprocess corpus part 1** : remove unwanted characters (e.g. punctuation), make the text
    lower case, map accented chars to non-accented (see notebook)
3. **Tokenise corpus** : all string docs become lists of tokens (words)
4. **Preprocess corpus part 2** : remove stop words, lemmatise
5. **Extract corpus vocabulary and compute IDF for each term** (token)


## Compute the TF matrix (ignore IDF for now)

- Doc1: {tomate,cebola, alho}Doc2: {manteiga, azeite, sal, pimenta} Doc 1 Doc Simple example
         - Tomate 1/3
            - Cebola 1/3
                  - Alho 1/3
   - Manteiga 0 1/
               - Azeite 0 1/
                     - Sal 0 1/
      - Pimenta 0 1/


The TF-IDF matrix

1. **Unified corpus representation:** documents represented in the same vocabulary vector
2. **Basic semantic inference becomes possible** : using e.g. cosine similarity (angle)
3. **Word-level representation** as matrix rows
4. **TF-IDF matrix can be factorised**
5. **Factorisation is useful** if we can interpret (extract semantics) from and , and we can!

```
A ≈ H × W
H W
```

Topic modelling as matrix factorisation


1. **Start with matrix A (TF-IDF)**
2. **Use some m** use the Latent Dirichlet Alloc **atrix factorisation method** ation (LDA). For topic models we
3. LDA factorises a matrix with **special statistical rules**.
4. These rules **distributed over topics** are a good match for **how words are naturally**
5. By onion **distribution we me** appears more often in recipes topic th **an term frequencies in topics** an in gener, e.g. al
    conversLDA maatches. tion, and this “more often” has statistical properties
6. LDA requires we say in advance the number of topics ( _k_ )


Topic modelling as matrix factorisation


1. **Start with matrix A (TF-IDF)**
2. **H is the loadings matrix:** how important are
    words in topic
3. **W is the scores matrix:** what proportion of
    each topic is in a document
4. We must be able to interpret these numbers
    correctly


Through and example

Topic model interpretation

```
0.3 0 0
0.3 0 0
0 0 0.
0 0.6 0.
0 0.6 0
0.3 0 0
```
```
milk
flour
lentils
tomato
beef
mozzarella
```
```
0.6 0 0 ...
0.4 0 0. ...
X 0 1 0. ...
```
### T1 T2 T

### D1 D2 D3 D

```
Topic T1 hflour, mozzaas high lorella, T2 hadings for for milk, as high tomato and
beef...
We cterms with high loan label topics considering the adings. For example T
is highly associated with “pizza”.
Document D1 is 60% T1 could be pasta bolognaaise, while nd 40% T2 (it
document D2 is 100% T3 (mtomato soup) aybe lentil
```
### T

### T

### T


